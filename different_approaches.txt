If the rule-based method is proving inadequate, there are a few alternative approaches that can handle the nuances of cricket commentary more effectively. Here are a few advanced techniques you could try:

### 1. **Named Entity Recognition (NER) with Transfer Learning**

Since your problem involves recognizing specific entities (e.g., batsman, bowler, action, etc.), an NER approach could work well. You can fine-tune a transformer model like BERT or RoBERTa for this task. This approach would allow the model to learn patterns from a dataset of cricket commentary and identify relevant entities.

#### Steps:
1. **Prepare a Dataset**: Gather a labeled dataset of cricket commentaries with the entities (bowler, batsman, shot type, etc.) tagged.
2. **Fine-tune a Pre-trained Model**: Use Hugging Face’s `transformers` library to fine-tune a model like `BERT` or `RoBERTa` on your dataset.
3. **Run NER on Commentary**: Once the model is fine-tuned, you can use it to identify the entities within the text.

Example with Hugging Face:

```python
from transformers import pipeline

# Load pre-trained NER model from Hugging Face
ner_pipeline = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")

# Example commentary
commentary = "Saqib Mahmood to Babar Azam, short ball, pulled to midwicket for Six"

# Run NER
entities = ner_pipeline(commentary)

# Print results
for entity in entities:
    print(f"Entity: {entity['word']}, Label: {entity['entity']}")
```

You can replace `"dbmdz/bert-large-cased-finetuned-conll03-english"` with a fine-tuned model on cricket commentary if you create one.

---

### 2. **Custom Sequence Labeling with CRF (Conditional Random Fields)**

Another powerful approach for labeling sequences (like commentary) is using **Conditional Random Fields (CRF)**. CRF works well for tasks where labels depend on the surrounding context, which is important in cricket commentary (e.g., `six` often follows a `pull` or `hook` shot). You can extract features for each word (POS tags, neighboring words, etc.) and feed them into a CRF model.

#### Steps:
1. **Feature Extraction**: Use spaCy to extract features like part-of-speech tags, word shapes, and dependency relations.
2. **Train a CRF Model**: Use `sklearn-crfsuite` to train a CRF model on a labeled dataset.
3. **Inference**: Once trained, the CRF model can predict the labels for unseen commentary.

Example of using CRF for token classification:

```python
import sklearn_crfsuite
from sklearn_crfsuite import metrics

# Assume X_train and y_train are the feature and label sets prepared earlier

# Initialize the CRF model
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
    max_iterations=100,
    all_possible_transitions=False
)

# Train the CRF model
crf.fit(X_train, y_train)

# Make predictions on new commentary data
y_pred = crf.predict(X_test)

# Evaluate the performance
metrics.flat_f1_score(y_test, y_pred, average='weighted')
```

The CRF model works well in sequence prediction tasks, especially where the context influences the labels.

---

### 3. **Dependency Parsing for Contextual Understanding**

If the structure of the commentary is regular (e.g., `"Bowler to Batsman, action"`), dependency parsing can help identify relationships between words, which can improve entity labeling and action identification. SpaCy’s dependency parsing can be particularly useful here.

For example, you can use dependency parsing to identify how different words (like `bowler`, `batsman`, and `shot type`) are related, which can help you improve the labeling logic.

```python
import spacy

# Load the spaCy model
nlp = spacy.load('en_core_web_sm')

# Example commentary
commentary = "Saqib Mahmood to Babar Azam, short ball, pulled to midwicket for Six"

# Process the commentary with spaCy
doc = nlp(commentary)

# Print dependency parsing information
for token in doc:
    print(f"{token.text} -> {token.dep_} -> {token.head.text}")
```

You can use dependency relations like `nsubj` (subject), `dobj` (object), or `prep` (prepositional relation) to create a more structured way of extracting actions, outcomes, and player roles.

---

### 4. **Using Machine Learning with Supervised Models (SVM, Random Forest, etc.)**

You can also use supervised machine learning models like **Support Vector Machines (SVM)**, **Random Forest**, or **Gradient Boosting** to classify tokens based on their features (POS, dependency, shape, etc.).

#### Steps:
1. **Extract Features**: Use spaCy or another NLP library to extract token-level features such as part-of-speech tags, word shapes, and dependency relations.
2. **Train a Classifier**: Use a classifier like SVM, Random Forest, or XGBoost to predict the label for each token based on its features.
3. **Evaluate**: Measure the classifier’s accuracy, precision, recall, and F1 score.

Example of using SVM:

```python
from sklearn import svm
from sklearn.model_selection import train_test_split

# Assume X (features) and y (labels) are prepared
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Train the SVM model
clf = svm.SVC(kernel='linear')
clf.fit(X_train, y_train)

# Predict on test data
y_pred = clf.predict(X_test)

# Evaluate
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```

With this approach, you can automate the labeling of tokens in commentary and let the model learn based on training data.

---

### 5. **Recurrent Neural Networks (RNN) or LSTM**

Given that commentary is a sequence of tokens with temporal dependencies, using a sequence model like an **LSTM (Long Short-Term Memory)** network can help capture these dependencies. LSTM models are commonly used for sequential tasks like language modeling or time series prediction.

#### Steps:
1. **Tokenize Commentary**: Tokenize the text and convert it to numerical representations (like word embeddings).
2. **Train LSTM**: Use the tokenized commentary to train an LSTM to predict the sequence of labels (e.g., bowler, shot type, run outcome).
3. **Predict**: The trained LSTM can predict the labels for new commentary.

Example (in TensorFlow or PyTorch):

```python
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize the commentary data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(commentary_data)
X = tokenizer.texts_to_sequences(commentary_data)

# Pad the sequences
X = pad_sequences(X, maxlen=100)

# Define an LSTM model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=100),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(num_labels, activation='softmax')
])

# Compile and train the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=10)
```

---

### Summary of Approaches:

1. **NER with Fine-tuned BERT/RoBERTa**: Train a transformer-based NER model for entity recognition.
2. **CRF**: Train a CRF model on token sequences for label prediction.
3. **Dependency Parsing**: Use spaCy’s dependency parsing to infer relationships between words.
4. **Supervised ML Models (SVM, Random Forest)**: Use classifiers to label tokens based on extracted features.
5. **LSTM/RNN**: Use sequence models to predict labels in commentary.

These approaches, particularly machine learning and deep learning methods, should handle the complexity of cricket commentary much better than rule-based systems. Fine-tuning the model on a domain-specific dataset (cricket commentary) will further improve performance.